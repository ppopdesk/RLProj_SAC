{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "annfromscratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN88jFOS/KBQY9/31yYhMkW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppopdesk/BCS-recruitment-task/blob/main/annfromscratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low level implementation of Neural Network with loss function as MSE and optimisation algorithm as Gradient Descent"
      ],
      "metadata": {
        "id": "08J44TqUyIpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVuKH8ginDHY"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Basic math function we are gonna use for the implementation"
      ],
      "metadata": {
        "id": "faWS7ewNyZpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(X):\n",
        "  return 1/(1+np.exp(-X))\n",
        "def relu(X):\n",
        "  return np.maximum(0,X)\n",
        "def sigmoid_prime(z):\n",
        "  return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "SYS42rXBdp5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network class contains all the methods for the various process happening in a neural network model : \n",
        "\n",
        "1) Layers initialisation\n",
        "\n",
        "2) Forward Propogation\n",
        "\n",
        "3) gradient Descent method\n",
        "\n",
        "4) Updating weights and biases based on one backward propogation step\n",
        "\n",
        "5) Backprop method\n",
        "\n",
        "6) Evaluation based on test set\n"
      ],
      "metadata": {
        "id": "PgflFMAvyf5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "  def __init__(self, sizes):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "  def forward_propogate(self,a,activation=None):\n",
        "    if activation:\n",
        "      if activation == 'relu':\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = relu(np.dot(w, a)+b)\n",
        "        return a\n",
        "      elif activation == 'sigmoid':\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    else:\n",
        "      for b, w in zip(self.biases, self.weights):\n",
        "        a = np.dot(w, a)+b\n",
        "      return a\n",
        "  def gradient_descent(self, training_data, eta, epochs):\n",
        "    n = len(training_data)\n",
        "    for j in range(epochs):\n",
        "      self.update_one_epoch(training_data, eta)\n",
        "      print(\"Epoch {0} complete\".format(j))\n",
        "  def update_one_epoch(self, training_data, eta):\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "    for (x, y) in zip(training_data[0],training_data[1]):\n",
        "      delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "      nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "      nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "    self.weights = [w-(eta/len(training_data))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "    self.biases = [b-(eta/len(training_data))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "  def backprop(self, x, y):\n",
        "      nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "      nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "      activation = x\n",
        "      activations = [x] # list to store all the activations, layer by layer\n",
        "      zs = [] # list to store all the z vectors, layer by layer\n",
        "      for (b, w) in zip(self.biases, self.weights):\n",
        "        z = np.dot(w, activation)+b\n",
        "        zs.append(z)\n",
        "        activation = sigmoid(z)\n",
        "        activations.append(activation)\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "          sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        for l in range(2, self.num_layers):\n",
        "          z = zs[-l]\n",
        "          sp = sigmoid_prime(z)\n",
        "          delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "          nabla_b[-l] = delta\n",
        "          nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "      return (nabla_b, nabla_w)\n",
        "  def evaluate(self, test_data):\n",
        "    test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in zip(training_data[0],training_data[1])]\n",
        "    return sum(int(x == y) for (x, y) in test_results)\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "    return (output_activations-y)"
      ],
      "metadata": {
        "id": "a3B7YvwSnHkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}